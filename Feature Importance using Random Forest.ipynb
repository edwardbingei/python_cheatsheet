{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a graph showing top 10 features as shown by Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into input and output\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision tree classifer object\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "\n",
    "# Train model\n",
    "model = clf.fit(X, y)\n",
    "\n",
    "# Plot the top 10 features based on its importance\n",
    "(pd.Series(model.feature_importances_, index=X.columns)\n",
    "   .nlargest(10)   # can adjust based on how many top features you want\n",
    "   .plot(kind='barh', figsize=[20,20])\n",
    "    .invert_yaxis()) # Ensures that the feature with the most importance is on top, in descending order\n",
    "\n",
    "plt.title('Top 10 Features derived by Random Forest', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternate way to create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision tree classifer object\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "\n",
    "# Train model\n",
    "model = clf.fit(X, y)\n",
    "\n",
    "# Calculate feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]  # largest to smallest\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [colnames[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=[20,20])\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.barh(range(X.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.yticks(range(X.shape[1]), names)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use SelectFromModel to identify features which have greater importance than the mean importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify random forest instance, indicate the number of trees\n",
    "# Use SelectFromModel to automatically select the features\n",
    "\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=45, threshold=.1)\n",
    "sel.fit(X_train, y_train)\n",
    "\n",
    "# See which features are important. Those with value of True are features whose\n",
    "# importance is greater than mean importance.\n",
    "sel.get_support()\n",
    "\n",
    "# Make a list and count selected features\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "\n",
    "# Get names of the features\n",
    "print(selected_feat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
